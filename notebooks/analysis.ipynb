{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c61489",
   "metadata": {},
   "source": [
    "# Bengali ASR Model Analysis\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading a trained model\n",
    "2. Running inference on sample audio\n",
    "3. Visualizing errors (WER/CER)\n",
    "4. Analyzing error patterns\n",
    "\n",
    "**Author:** BRAC Data Science Team  \n",
    "**Date:** October 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fdb868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if needed)\n",
    "# !pip install transformers datasets librosa soundfile jiwer matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration\n",
    ")\n",
    "from jiwer import wer, cer\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d26736",
   "metadata": {},
   "source": [
    "## 1. Load Model\n",
    "\n",
    "Load a trained Wav2Vec2 or Whisper model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6410a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = \"../models/wav2vec2_bengali/checkpoint-best\"\n",
    "MODEL_TYPE = \"wav2vec2\"  # or \"whisper\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading {MODEL_TYPE} model from {MODEL_PATH}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Load model and processor\n",
    "if MODEL_TYPE == \"wav2vec2\":\n",
    "    processor = Wav2Vec2Processor.from_pretrained(MODEL_PATH)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "elif MODEL_TYPE == \"whisper\":\n",
    "    processor = WhisperProcessor.from_pretrained(MODEL_PATH)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model type: {MODEL_TYPE}\")\n",
    "\n",
    "model.eval()\n",
    "print(\"✓ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56425cfb",
   "metadata": {},
   "source": [
    "## 2. Inference on Sample Audio\n",
    "\n",
    "Transcribe a sample Bengali audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ba1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample audio\n",
    "# TODO: Replace with actual audio file path\n",
    "SAMPLE_AUDIO_PATH = \"../data/samples/sample_bengali.wav\"\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(SAMPLE_AUDIO_PATH):\n",
    "    print(f\"⚠️ Sample audio not found at {SAMPLE_AUDIO_PATH}\")\n",
    "    print(\"Please provide a sample audio file or download from OpenSLR.\")\n",
    "else:\n",
    "    # Load and display audio\n",
    "    audio, sr = librosa.load(SAMPLE_AUDIO_PATH, sr=16000)\n",
    "    print(f\"Audio duration: {len(audio) / sr:.2f} seconds\")\n",
    "    print(f\"Sample rate: {sr} Hz\")\n",
    "    \n",
    "    # Display audio player\n",
    "    display(Audio(audio, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9559fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path, model, processor, device, model_type=\"wav2vec2\"):\n",
    "    \"\"\"Transcribe audio file.\"\"\"\n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "    \n",
    "    if model_type == \"wav2vec2\":\n",
    "        # Process with Wav2Vec2\n",
    "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs.input_values.to(device)).logits\n",
    "        \n",
    "        # Decode\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(predicted_ids)[0]\n",
    "        \n",
    "        # Calculate confidence\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        max_probs = torch.max(probs, dim=-1).values\n",
    "        confidence = float(max_probs.mean().cpu())\n",
    "        \n",
    "    else:  # whisper\n",
    "        # Process with Whisper\n",
    "        input_features = processor(\n",
    "            audio,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(input_features)\n",
    "        \n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        confidence = 0.9  # Whisper doesn't provide confidence\n",
    "    \n",
    "    return transcription, confidence\n",
    "\n",
    "\n",
    "# Transcribe sample\n",
    "if os.path.exists(SAMPLE_AUDIO_PATH):\n",
    "    transcript, confidence = transcribe_audio(\n",
    "        SAMPLE_AUDIO_PATH, model, processor, DEVICE, MODEL_TYPE\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRANSCRIPTION RESULT\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Transcript: {transcript}\")\n",
    "    print(f\"Confidence: {confidence:.2%}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b98ec9c",
   "metadata": {},
   "source": [
    "## 3. Evaluate on Test Set\n",
    "\n",
    "Calculate WER and CER on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b86273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "TEST_TSV = \"../data/processed/test.tsv\"\n",
    "\n",
    "if os.path.exists(TEST_TSV):\n",
    "    test_df = pd.read_csv(TEST_TSV, sep='\\t')\n",
    "    print(f\"Loaded {len(test_df)} test samples\")\n",
    "    \n",
    "    # Sample first 10 for quick evaluation (change to evaluate all)\n",
    "    test_df_sample = test_df.head(10)\n",
    "    \n",
    "    # Transcribe\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for idx, row in test_df_sample.iterrows():\n",
    "        try:\n",
    "            pred, _ = transcribe_audio(row['path'], model, processor, DEVICE, MODEL_TYPE)\n",
    "            predictions.append(pred)\n",
    "            references.append(row['transcript'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error on {row['path']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    wer_score = wer(references, predictions)\n",
    "    cer_score = cer(references, predictions)\n",
    "    \n",
    "    print(f\"\\nWord Error Rate (WER): {wer_score*100:.2f}%\")\n",
    "    print(f\"Character Error Rate (CER): {cer_score*100:.2f}%\")\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame({\n",
    "        'reference': references,\n",
    "        'prediction': predictions\n",
    "    })\n",
    "    \n",
    "    display(results_df.head())\n",
    "else:\n",
    "    print(f\"Test file not found: {TEST_TSV}\")\n",
    "    print(\"Please run preprocessing first: cd ../data && python preprocess.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8484fca",
   "metadata": {},
   "source": [
    "## 4. Visualize Errors\n",
    "\n",
    "Visualize error distribution and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-sample WER\n",
    "if 'results_df' in locals():\n",
    "    sample_wers = []\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        sample_wers.append(wer([ref], [pred]))\n",
    "    \n",
    "    # Plot WER distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0].hist(sample_wers, bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_xlabel('Word Error Rate')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('WER Distribution')\n",
    "    axes[0].axvline(np.mean(sample_wers), color='red', linestyle='--', label=f'Mean: {np.mean(sample_wers):.2%}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Box plot\n",
    "    axes[1].boxplot(sample_wers, vert=True)\n",
    "    axes[1].set_ylabel('Word Error Rate')\n",
    "    axes[1].set_title('WER Box Plot')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nError Statistics:\")\n",
    "    print(f\"Mean WER: {np.mean(sample_wers):.2%}\")\n",
    "    print(f\"Median WER: {np.median(sample_wers):.2%}\")\n",
    "    print(f\"Std Dev: {np.std(sample_wers):.2%}\")\n",
    "    print(f\"Min WER: {np.min(sample_wers):.2%}\")\n",
    "    print(f\"Max WER: {np.max(sample_wers):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78901ed2",
   "metadata": {},
   "source": [
    "## 5. Error Pattern Analysis\n",
    "\n",
    "Identify common transcription errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d24bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import difflib\n",
    "\n",
    "if 'results_df' in locals():\n",
    "    substitutions = []\n",
    "    deletions = []\n",
    "    insertions = []\n",
    "    \n",
    "    for ref, pred in zip(references, predictions):\n",
    "        ref_words = ref.split()\n",
    "        pred_words = pred.split()\n",
    "        \n",
    "        matcher = difflib.SequenceMatcher(None, ref_words, pred_words)\n",
    "        \n",
    "        for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "            if tag == 'replace':\n",
    "                for i, j in zip(range(i1, i2), range(j1, j2)):\n",
    "                    if i < len(ref_words) and j < len(pred_words):\n",
    "                        substitutions.append((ref_words[i], pred_words[j]))\n",
    "            elif tag == 'delete':\n",
    "                for i in range(i1, i2):\n",
    "                    if i < len(ref_words):\n",
    "                        deletions.append(ref_words[i])\n",
    "            elif tag == 'insert':\n",
    "                for j in range(j1, j2):\n",
    "                    if j < len(pred_words):\n",
    "                        insertions.append(pred_words[j])\n",
    "    \n",
    "    print(\"\\nMost Common Substitutions:\")\n",
    "    print(\"=\"*60)\n",
    "    for (ref_word, pred_word), count in Counter(substitutions).most_common(10):\n",
    "        print(f\"{ref_word:20s} → {pred_word:20s} ({count} times)\")\n",
    "    \n",
    "    print(\"\\nMost Common Deletions:\")\n",
    "    print(\"=\"*60)\n",
    "    for word, count in Counter(deletions).most_common(10):\n",
    "        print(f\"{word:20s} (deleted {count} times)\")\n",
    "    \n",
    "    print(\"\\nMost Common Insertions:\")\n",
    "    print(\"=\"*60)\n",
    "    for word, count in Counter(insertions).most_common(10):\n",
    "        print(f\"{word:20s} (inserted {count} times)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06116f3a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Loading and using a trained Bengali ASR model\n",
    "- Transcribing audio samples\n",
    "- Calculating WER and CER metrics\n",
    "- Analyzing error patterns\n",
    "\n",
    "**Next Steps:**\n",
    "1. Collect more training data in error-prone areas\n",
    "2. Apply targeted data augmentation\n",
    "3. Fine-tune on BRAC dialect data\n",
    "4. Implement language model for better accuracy"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
